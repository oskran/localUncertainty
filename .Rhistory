for (i in seq_along(toBeReplaced)) {
database[database$paper == toBeReplaced[i], 3] <- toReplace[i]
}
uniquePapers <- c(uniquePapers, length(unique(database$paper)))
# Remove papers with only one observation
oneObs <- database %>% group_by(paper) %>% count() %>% filter(n == 1)
database <- database %>% filter(!paper %in% oneObs$paper)
uniquePapers <- c(uniquePapers, length(unique(database$paper)))
database %>% filter(year > 2013) %>% distinct(paper)
database <- database %>% filter(year <= 2013)
data.frame(operation = c("Original data", "Removed non-Norwegian", "Combined series", "Removed one obs."), uniquePapers)
database <- database %>% filter(year >= startYear, year <= endYear)
dateDf <- data.frame(seq(from = as.Date("2000/1/1"), to = as.Date("2013/12/31"), by = "month"),
rep(seq(from = startYear, to = endYear), each = 12),
rep(seq(from = 1, to = 12), noYears))
names(dateDf) <- c("date", "year", "month")
database <- inner_join(database, dateDf)
coverage <- read.csv("Data/SpredningOgHusstandsdekningKommune_1558367839680.csv",
row.names = NULL, sep = ";", encoding = "ISO 8859-1",
stringsAsFactors = F)
coverage <- coverage[, -(8:9)]
names(coverage) <- c("paper", "municipality", "municipality.no.", "households", "dekning", "spread", "year")
coverage$paper <- tolower(coverage$paper)
# Replace Norwegian characters with ae, o or aa
subt <- data.frame(original = c("Ã¦", "Ã¸", "Ã¥", "Ã", "Ã", "Ã"), repl = c("ae", "o", "aa", "AE", "O", "AA"))
for (i in 1:3) {
coverage$paper <- gsub(subt[i, 1], subt[i, 2], coverage$paper)
}
for (i in 1:6) {
coverage$municipality <- gsub(subt[i, 1], subt[i, 2], coverage$municipality)
}
# Replaces comma with punctuation mark
coverage$dekning <- gsub(",", ".", coverage$dekning, fixed = T)
# Removes percentage sign
coverage$dekning <- gsub("%", "", coverage$dekning, fixed = T)
# Removes blank spaces
coverage$spread <- gsub(" ", "", coverage$spread, fixed = T)
coverage$households <- gsub(" ", "", coverage$households, fixed = T)
# Converts to numeric
coverage[, 4] <- as.numeric(coverage[, 4])
coverage[, 5] <- as.numeric(coverage[, 5])
coverage[, 6] <- as.numeric(coverage[, 6])
# Subset coverage year
coverage[, 7] <- substr(coverage[, 7], 1, 4)
coverage$year <- as.numeric(coverage$year)
# Filter year less or equal to 2014:
coverage <- coverage %>% filter(year <= 2014)
# Add "0" to beginning of munucipality numbers with only 3 digits
coverage <- coverage %>% mutate(municipality.no. = ifelse(nchar(municipality.no.) == 3, paste0("0", municipality.no.), municipality.no.))
# Remove " " and "-" from paper names
unwantedChr <- c(" ", "-")
for (i in seq_along(unwantedChr)) {
coverage$paper <- gsub(unwantedChr[i], "", coverage$paper)
}
database %>% arrange(desc(uncertainty)) %>% head(n = 5)
database %>%
mutate(uncertaintyPerWord = uncertainty / total) %>%
arrange(desc(uncertaintyPerWord)) %>%
head(n = 5)
database %>% filter(uncertainty == 0) %>% nrow()
paste0(format((database %>% filter(uncertainty == 0) %>% nrow() * 100) / database %>% nrow(), digits = 3), "%") # Ratio of paper/months with no unc. words
# Unique newspapers
database %>% filter(!total == 0) %>% group_by(date) %>% distinct(paper) %>% count() %>%
ggplot(aes(date, n)) +
geom_line() +
scale_x_date(breaks = as.Date(c("2000-01-01", "2002-01-01", "2004-01-01", "2006-01-01", "2008-01-01", "2010-01-01", "2012-01-01")), date_labels = "%Y") +
theme_classic()
# Total words
database %>% group_by(date) %>% summarise(total = sum(total)) %>%
ggplot(aes(date, total)) +
geom_line() +
scale_x_date(breaks = as.Date(c("2000-01-01", "2002-01-01", "2004-01-01", "2006-01-01", "2008-01-01", "2010-01-01", "2012-01-01")), date_labels = "%Y") +
theme_classic()
coverage %>% group_by(year) %>% summarise(spread = sum(spread)) %>%
ggplot(aes(year, spread)) +
geom_line() +
theme_classic()
coverage %>% group_by(year) %>% distinct(paper) %>% count() %>%
ggplot(aes(year, n)) +
geom_line() +
theme_classic()
# Match papers that have the exact same name.
papersCoverage <- data.frame(paper = tolower(unique(coverage$paper)), stringsAsFactors = FALSE)
papersDatabase <- data.frame(paper = unique(database$paper), stringsAsFactors = FALSE)
matchedPapers <- inner_join(papersCoverage, papersDatabase)
matchedPapers <- data.frame(s1name = matchedPapers$paper, s2name = matchedPapers$paper, stringsAsFactors = FALSE)
papersCoverage <- papersCoverage %>% filter(!paper %in% matchedPapers$s1name)
papersDatabase <- papersDatabase %>% filter(!paper %in% matchedPapers$s1name)
# Match using a variety of string distance measurements
emptyVector1 <- c(rep(0, length(papersDatabase$paper)))
paperNames <- data.frame(papersDatabase$paper, emptyVector1, stringsAsFactors = F)
names(paperNames) <- "name"
emptyVector2 <- c(rep(0, length(papersCoverage$paper)))
namesDekning <- data.frame(papersCoverage$paper, emptyVector2, stringsAsFactors = F)
names(namesDekning)[1] <- "name"
#distance.methods<-'jw'
distance.methods<-c('osa','lv','dl','lcs','qgram','cosine','jaccard','jw')
dist.methods<-list()
for (m in 1:length(distance.methods)) {
dist.name.enh<-matrix(NA, ncol = length(paperNames$name), nrow = length(namesDekning$name))
for (i in 1:length(paperNames$name)) {
for (j in 1:length(namesDekning$name)) {
dist.name.enh[j, i] <- stringdist(tolower(paperNames[i, ]$name),
tolower(namesDekning[j, ]$name), method = distance.methods[m])
#adist.enhance(paperNames[i,]$name,namesDekning[j,]$name)
}
}
dist.methods[[distance.methods[m]]] <- dist.name.enh
}
match.s1.s2.enh <- NULL
for (m in 1:length(dist.methods)) {
dist.matrix <- as.matrix(dist.methods[[distance.methods[m]]])
min.name.enh <- apply(dist.matrix, 1, base::min)
for (i in 1:nrow(dist.matrix)) {
s2.i <- match(min.name.enh[i], dist.matrix[i, ])
s1.i <- i
match.s1.s2.enh <- rbind(data.frame(s2.i = s2.i, s1.i = s1.i, s2name = paperNames[s2.i, ]$name,
s1name = namesDekning[s1.i, ]$name, adist = min.name.enh[i],
method = distance.methods[m], stringsAsFactors = FALSE), match.s1.s2.enh)
}
}
# Results from stringdist matching
matched.names.matrix <- dcast(match.s1.s2.enh, s2.i + s1.i + s2name + s1name ~ method, value.var = "adist")
matched.names.matrix <- matched.names.matrix %>% arrange(jw)
# Extract matches based on limiting values set for each string distance measure
matched2 <- matched.names.matrix %>%
filter(jw <= 0.13 |
jaccard <= 0.08 |
cosine <= 0.072 |
lcs <= 3 |
dl <= 2 |
lv <= 2 |
osa <= 2) %>%
select(s2name, s1name)
matchedPapers <- rbind(matchedPapers, matched2)
matched.names.matrix <- matched.names.matrix %>%
filter(!s2name %in% matched2$s2name | !s1name %in% matched.names.matrix$s1name)
# Manually pick out the last matching papers.
matchedPapers <- rbind(matchedPapers, matched.names.matrix[c(1, 3, 5, 8, 11, 13, 76), c(3, 4)])
nrow(matchedPapers)
coverage <- matchedPapers %>% rename(paper = s1name) %>% left_join(coverage) %>% select(-s2name)
database <- matchedPapers %>% rename(paper = s2name) %>% left_join(database) %>% select(-paper) %>% rename(paper = s1name)
counties <- data.frame(county = c("Akershus", "Aust-Agder", "Buskerud", "Finnmark - Finnm?rku", "Hedmark", "Hordaland", "M?re og Romsdal", "Nordland", "Oppland",  "Oslo", "Rogaland", "Sogn og Fjordane", "Telemark", "Troms - Romsa", "Tr?ndelag", "Vest-Agder", "Vestfold", "?stfold", "Tr?ndelag", "Tr?ndelag"), from = c(0200, 0900, 0600, 2000, 0400, 1200, 1500, 1800, 0500, 0300, 1100, 1400, 0800, 1900, 5000, 1000, 700, 0100, 01600, 01700), stringsAsFactors = F)
coverage$municipality.no. <- as.numeric(coverage$municipality.no.)
coverageNumbers <- coverage %>% distinct(municipality.no.)
z <- character(nrow(coverageNumbers))
for (j in 1:nrow(coverageNumbers)){
for (i in 1:nrow(counties)){
if((coverageNumbers[j, 1] > counties[i, 2]) & (coverageNumbers[j, 1] < counties[i, 2] + 99)) {
z[j] <- counties[i, 1]
}
}
}
coverageNumbers$county <- z
coverage <- inner_join(coverage, coverageNumbers, by = "municipality.no.")
possiblePapers <- tibble(paper = rep(rep(unique(database$paper)), each = 12*(noYears)),
date = rep(seq(from = as.Date("2000/1/1"), to = as.Date("2013/12/31"), by = "month"), length(unique(database$paper))))
papersPerMonth <- database %>%
group_by(date, paper) %>%
summarise(count = n()) %>%
mutate(data = 1)
tileData <- right_join(papersPerMonth, possiblePapers)
tileData[is.na(tileData)] <- 0
tileData <- inner_join(tileData, papersPerMonth %>% group_by(paper) %>% summarise(noOfMonths = sum(data)) %>% ungroup())
tileData <- tileData %>% arrange(noOfMonths)
tileData$paper <- factor(tileData$paper, levels = (unique(tileData$paper)))
graphData <- tileData %>% #filter(year %in% 2000:2007) %>%
ggplot(aes(date, paper)) +
scale_y_discrete(expand=c(0,0)) +
scale_x_date(expand = c(0,0), date_breaks = "1 year", date_labels = "%Y") +
geom_tile(aes(fill = data, color = data), size = 1, show.legend = FALSE) +
theme(axis.title.x=element_blank(),
axis.title.y=element_blank())
dataMonths2000 <- database %>%
filter(year %in% 2000:2007) %>%
distinct(paper, date) %>%
group_by(paper) %>%
count(name = "freq") %>%
arrange(desc(freq)) %>%
ungroup() %>%
filter(freq * 100 / max(freq) > 80) # Select papers with < 20% missing data
dataMonths2008 <- database %>%
filter(year %in% 2008:2011) %>%
distinct(paper, date) %>%
group_by(paper) %>%
count(name = "freq") %>%
arrange(desc(freq)) %>%
ungroup() %>%
filter(freq * 100 / max(freq) > 80) # Select papers with < 20% missing data
database <- rbind(database %>% filter(paper %in% unique(dataMonths2000$paper), year %in% 2000:2007),
database %>% filter(paper %in% unique(dataMonths2008$paper), year %in% 2008:2011))
database$index <- c(rep(2000, nrow(database[database$year %in% 2000:2007, ])),
rep(2008, nrow(database[database$year %in% 2008:2011, ])))
coveragePerYear <- coverage %>% group_by(paper, county, year) %>% summarise(spread = sum(spread)) %>% ungroup()
paperNames <- unique(database$paper)
# Linear approximation of year/county pairs with missing data:
linApprox <- tibble(paper = rep(paperNames, each = 18*(2014-2000+1)),
county = rep(unique(coverage$county), each = (2014-2000+1), length(paperNames)),
year = rep(2000:2014, length(paperNames)*18))
coveragePerYear <- left_join(linApprox, coveragePerYear)
coveragePerYear <- coveragePerYear %>% arrange(paper, county, year)
coveragePerYear <- coveragePerYear %>% group_by(paper, county) %>% mutate(spread = na.approx(spread, na.rm = FALSE, rule = 2)) %>% ungroup() # For each group, NAs at the left or right side are set equal to the closest year.
# Add rows with sum of spread in all counties
totalCoveragePerYear <- coveragePerYear %>% group_by(year, paper) %>% summarise(spread = sum(spread, na.rm = TRUE)) %>% ungroup()
totalCoveragePerYear$county <- "Total"
coveragePerYear <- rbind(coveragePerYear, totalCoveragePerYear)
# Add column with total yearly coverage per county
coveragePerYear <- inner_join(
coveragePerYear,
coveragePerYear %>% group_by(year, county) %>% summarise(totCoverage = sum(spread, na.rm = TRUE)) %>% ungroup())
# Calculate weights
coveragePerYear$weigth <- coveragePerYear$spread / coveragePerYear$totCoverage
database <- inner_join(database, coveragePerYear)
database$uncertaintyPerWord <- database$uncertainty / database$total
# Standarize each weighted newspaper-level series to unit standard deviation
database <- database %>% group_by(paper, county, index) %>% mutate(stdr = sd(uncertaintyPerWord, na.rm = T)) %>% ungroup()
database <- database %>% group_by(paper, county, index) %>% mutate(uncertaintySt = uncertaintyPerWord / stdr) %>% ungroup() #Uncertainty per word divided by sd
database$uncertaintyWeighted <- database$uncertaintyPerWord * database$weigth # Uncertainty per word per newspaper sold, per newspaper.
database$uncertaintyStWeighted <- database$uncertaintySt * database$weigth
# Make collumns with average and sum of all the newspapers by month
database <- database %>%
group_by(county, date, index) %>%
summarise(totalSum = sum(total, na.rm = T), # Total words per month
usikkerhetSum = sum(uncertainty, na.rm = T), # Number of uncertainty words per month
uncertaintyPerWordSum = sum(uncertaintyPerWord, na.rm = T), # Number of uncertainty words per word per month
uncertaintyWeightedSum = sum(uncertaintyWeighted, na.rm = T), # Uncertainty per paper, per month
total = mean(total, na.rm = T), # Mean words per newspaper per month
usikkerhet = mean(uncertainty, na.rm = T), # Mean uncertainty words per newspaper per month
uncertaintyPerWord = mean(uncertaintyPerWord, na.rm = T), # Mean uncertainty words per word, per newspaper per              month
uncertaintyStWeighted = mean(uncertaintyStWeighted, na.rm = T),
uncertaintyWeighted = mean(uncertaintyWeighted, na.rm = T)) %>% ungroup() # Mean uncertainty per paper, per paper, per month
# Normalize to mean 100 for St
seriesMeanSt <- database %>% group_by(county, index) %>% summarise(mean(uncertaintyStWeighted, na.rm = T)) %>% ungroup()
database <- inner_join(database, seriesMeanSt)
database$uncertaintyNormSt <- database$uncertaintyStWeighted * (100/database$`mean(uncertaintyStWeighted, na.rm = T)`)
# Normalize to mean 100
seriesMean <- database %>% group_by(county, index) %>% summarise(mean(uncertaintyWeighted, na.rm = T)) %>% ungroup()
database <- inner_join(database, seriesMean)
database$uncertaintyNorm <- database$uncertaintyWeighted * (100/database$`mean(uncertaintyWeighted, na.rm = T)`)
# Rolling mean
database <-
database %>%
group_by(county, index) %>%
mutate(rollingMeanSt = c(rep(NA, 9), rollmean(uncertaintyNormSt, 10, align = "right")),
rollingMean = c(rep(NA, 9), rollmean(uncertaintyNorm, 10, align = "right"))) %>% ungroup()
oilPrice <- readRDS("Data/oilPrice.rds")
unemployment <- readRDS("Data/unemployment.rds")
migration <- readRDS("Data/migration.rds")
worldUncertainty <- readRDS("Data/worldUncertainty.rds")
# List of historical uncertainty events from Larsen paper.
uncertaintyEvents <- read.csv("Data/uncertaintyEvents.csv", sep = ";", stringsAsFactors = F)
uncertaintyEvents$date <- as.Date(uncertaintyEvents$date, format = "%d.%m.%Y")
# Create series with oil price from 2000 to 2013
prove <- left_join(database, oilPrice)
# Create series with unemployemt for each county from 2000 to 2013
database <- left_join(database, unemployment[, c("date", "county", "unemployed")])
Skaler <- function(x, y){
((x - min(x)) / (max(x) - min(x))) * (max(y) - min(y)) + min(y)
}
for (i in seq_along(unique(database$county))) {
database[database$county == unique(database$county)[i], "unempScaler"] <- Skaler(database[database$county == unique(database$county)[i], "unemployed", ], database[database$county == unique(database$county)[i], "uncertaintyNorm"])
}
database <- left_join(database, worldUncertainty)
database[,"worldIndex"] <-  Skaler(database$worldIndex, database$uncertaintyNorm)
### Graphs
```{r}
### Graphs
```{r}
database %>% filter(index == 2000, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line() +
#geom_line(aes(dateMonth, rollingMean)) +
#geom_line(aes(dateMonth, rollingMean)) +
geom_vline(xintercept = uncertaintyEvents[c(12, 14, 15, 16, 17, 18, 19), 2],
show.legend = T, linetype = "dotted", size = 1) +
theme_classic() +
theme(axis.title = element_blank())
database %>% filter(index == 2008, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line() +
# geom_line(aes(dateMonth, worldIndex)) +
geom_vline(xintercept = uncertaintyEvents[c(20, 21, 23, 25), 2],
show.legend = T, linetype = "dotted", size = 1) +
theme_classic() +
theme(axis.title = element_blank())
database %>% filter(index == 2000, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line(color = "gray") +
#geom_line(aes(dateMonth, rollingMean)) +
geom_line(aes(date, worldIndex)) +
theme_classic() +
theme(axis.title = element_blank())
database2008 %>% filter(index == 2000, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line(color = "gray") +
geom_line(aes(date, worldIndex)) +
theme_classic() +
theme(axis.title = element_blank())
database %>% filter(index == 2000, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line(color = "gray") +
geom_line(aes(date, worldIndex)) +
theme_classic() +
theme(axis.title = element_blank())
database %>% filter(index == 2000) %>%
ggplot(aes(x = date, y = uncertaintyNorm)) +
geom_line() +
facet_wrap(~county) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_blank())
database %>% filter(index == 2008) %>%
ggplot(aes(x = date, y = uncertaintyNorm)) +
geom_line() +
facet_wrap(~county) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_blank())
database %>% filter(index == 2000) %>%
ggplot(aes(x = date, y = uncertaintyNorm)) +
geom_line() +
facet_wrap(~county) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_blank())
database %>% filter(index == 2008, county == "Total") %>%
ggplot(aes(date, uncertaintyNorm)) +
geom_line() +
# geom_line(aes(dateMonth, worldIndex)) +
geom_vline(xintercept = uncertaintyEvents[c(20, 21, 23, 25), 2],
show.legend = T, linetype = "dotted", size = 1) +
theme_classic() +
theme(axis.title = element_blank())
# List of historical uncertainty events from Larsen paper.
uncertaintyEvents <- read.csv("Data/uncertaintyEvents.csv", sep = ";", stringsAsFactors = F)
uncertaintyEvents$date <- as.Date(uncertaintyEvents$date, format = "%d.%m.%Y")
View(uncertaintyEvents)
database %>% filter(index == 2000, !county == "Total") %>%
ggplot(aes(x = date, y = uncertaintyNorm)) +
geom_line() +
facet_wrap(~county) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_blank())
graphData <- tileData %>% #filter(year %in% 2000:2007) %>%
ggplot(aes(date, paper)) +
scale_y_discrete(expand=c(0,0)) +
scale_x_date(expand = c(0,0), date_breaks = "1 year", date_labels = "%Y") +
geom_tile(aes(fill = data, color = data), size = 1, show.legend = FALSE) +
scale_fill_manual(values = c("gray", "black")) +
theme(axis.title.x=element_blank(),
axis.title.y=element_blank())
knitr::opts_chunk$set(echo = TRUE)
# !diagnostics off
options(scipen=999)
library(httr) # GET, content
library(tidyverse)
library(stringdist)
library(reshape2)
library(jsonlite)
library(pxweb) # Statistics Norway API
library(zoo) # Linear approximation
rm(list = ls())
startYear <- 2000
endYear <- 2013
noYears <- endYear - startYear + 1
# factor: by how many positions wildcards are expanded
# limit:  top #limit results,
# freq_lim: discard results with less than #freq_lim hits
unc_words <- c("usikkerhet*", "uvisse*", "uvissa*")
key_list <- c()
for (i in seq_along(unc_words)) {
querylist =
list(
word = unc_words[i],
corpus = "avis",
factor = "20",
limit = "1000",
freq_lim = "2"
)
query_result = GET(url = "https://api.nb.no/ngram/wildcards", query = querylist)
key_list = c(key_list, names(content(query_result)))
}
sort(key_list)
database <- readRDS("Data/monthlyData.rds")
database$paper <- as.character(database$paper)
names(database)[6] <- "uncertainty"
database$paper <- as.character(database$paper)
database <- database %>% filter(!total == 0)
uniquePapers <- length(unique(database$paper))
# Remove non-Norwegian papers:
database <- database %>% filter(!paper %in% c("avvir", "upstream", "tradewinds", "recharge", "assu", "minaigi", "ruijankaiku"))
uniquePapers <- c(uniquePapers, length(unique(database$paper)))
# Combines newspapers series that contains the same paper under different names:
toReplace <- c("dagsavisen", "dagsavisen", "dagsavisen", "bergensavisen", "gudbrandsdoelendagning", "gudbrandsdoelendagning", "gudbrandsdoelendagning", "gudbrandsdoelendagning",
"tronderavisa", "dagensnaeringsliv", "smaalenenesavis", "nytid", "samholdvelgeren", "helgelandarbeiderblad", "telemarksavisa", "budstikkaforaskerogb", "dagenmagazinet",
"drammenstidende", "drammenstidende", "fiskeribladetfiskare", "tromsfolkeblad", "friheten", "gjengangeren", "hardangerfolkeblad", "lofotposten",
"telemarksavisa", "eidsvollullensakerbl", "hardangerfolkeblad", "avisanordland", "avisanordland", "fiskeribladetfiskare")
toBeReplaced <- c("arbeiderbladetoslo", "arbeiderbladet", "dagsavisenarbeiderbladet", "bergensarbeiderblad", "gudbrandsdolen", "dagningen", "gudbrandsdoelen", "gudbrandsdolendagnin",
"nordtrondelaginntrondelagen", "norgeshandelsogsjoefartstidende", "oevresmaalenene", "orientering", "samholdgjoevik", "helgelandarbeiderbla", "telemarkarbeiderblad", "askerogbaerumsbudstikke", "dagenbergen",
"drammenstidendeogbuskerudblad", "drammenstidendeogbus", "fiskeribladet", "tffolkebladet", "friheten2", "gjengangeren2", "hardangerfolkeblad", "lofotposten2",
"telemarkarbeiderblad", "eidsvoldblad", "hardanger", "nordlandsframtid", "nordlandsposten", "fiskaren")
replacement <- data.frame(toReplace, toBeReplaced)
for (i in seq_along(toBeReplaced)) {
database[database$paper == toBeReplaced[i], 3] <- toReplace[i]
}
uniquePapers <- c(uniquePapers, length(unique(database$paper)))
# Remove papers with only one observation
oneObs <- database %>% group_by(paper) %>% count() %>% filter(n == 1)
database <- database %>% filter(!paper %in% oneObs$paper)
uniquePapers <- c(uniquePapers, length(unique(database$paper)))
database %>% filter(year > 2013) %>% distinct(paper)
database <- database %>% filter(year <= 2013)
data.frame(operation = c("Original data", "Removed non-Norwegian", "Combined series", "Removed one obs."), uniquePapers)
database <- database %>% filter(year >= startYear, year <= endYear)
dateDf <- data.frame(seq(from = as.Date("2000/1/1"), to = as.Date("2013/12/31"), by = "month"),
rep(seq(from = startYear, to = endYear), each = 12),
rep(seq(from = 1, to = 12), noYears))
names(dateDf) <- c("date", "year", "month")
database <- inner_join(database, dateDf)
coverage <- read.csv("Data/SpredningOgHusstandsdekningKommune_1558367839680.csv",
row.names = NULL, sep = ";", encoding = "ISO 8859-1",
stringsAsFactors = F)
coverage <- coverage[, -(8:9)]
names(coverage) <- c("paper", "municipality", "municipality.no.", "households", "dekning", "spread", "year")
coverage$paper <- tolower(coverage$paper)
# Replace Norwegian characters with ae, o or aa
subt <- data.frame(original = c("Ã¦", "Ã¸", "Ã¥", "Ã", "Ã", "Ã"), repl = c("ae", "o", "aa", "AE", "O", "AA"))
for (i in 1:3) {
coverage$paper <- gsub(subt[i, 1], subt[i, 2], coverage$paper)
}
for (i in 1:6) {
coverage$municipality <- gsub(subt[i, 1], subt[i, 2], coverage$municipality)
}
# Replaces comma with punctuation mark
coverage$dekning <- gsub(",", ".", coverage$dekning, fixed = T)
# Removes percentage sign
coverage$dekning <- gsub("%", "", coverage$dekning, fixed = T)
# Removes blank spaces
coverage$spread <- gsub(" ", "", coverage$spread, fixed = T)
coverage$households <- gsub(" ", "", coverage$households, fixed = T)
# Converts to numeric
coverage[, 4] <- as.numeric(coverage[, 4])
coverage[, 5] <- as.numeric(coverage[, 5])
coverage[, 6] <- as.numeric(coverage[, 6])
# Subset coverage year
coverage[, 7] <- substr(coverage[, 7], 1, 4)
coverage$year <- as.numeric(coverage$year)
# Filter year less or equal to 2014:
coverage <- coverage %>% filter(year <= 2014)
# Add "0" to beginning of munucipality numbers with only 3 digits
coverage <- coverage %>% mutate(municipality.no. = ifelse(nchar(municipality.no.) == 3, paste0("0", municipality.no.), municipality.no.))
# Remove " " and "-" from paper names
unwantedChr <- c(" ", "-")
for (i in seq_along(unwantedChr)) {
coverage$paper <- gsub(unwantedChr[i], "", coverage$paper)
}
database %>% arrange(desc(uncertainty)) %>% head(n = 5)
database %>%
mutate(uncertaintyPerWord = uncertainty / total) %>%
arrange(desc(uncertaintyPerWord)) %>%
head(n = 5)
database %>% filter(uncertainty == 0) %>% nrow()
paste0(format((database %>% filter(uncertainty == 0) %>% nrow() * 100) / database %>% nrow(), digits = 3), "%") # Ratio of paper/months with no unc. words
# Unique newspapers
database %>% filter(!total == 0) %>% group_by(date) %>% distinct(paper) %>% count() %>%
ggplot(aes(date, n)) +
geom_line() +
scale_x_date(breaks = as.Date(c("2000-01-01", "2002-01-01", "2004-01-01", "2006-01-01", "2008-01-01", "2010-01-01", "2012-01-01")), date_labels = "%Y") +
theme_classic()
# Total words
database %>% group_by(date) %>% summarise(total = sum(total)) %>%
ggplot(aes(date, total)) +
geom_line() +
scale_x_date(breaks = as.Date(c("2000-01-01", "2002-01-01", "2004-01-01", "2006-01-01", "2008-01-01", "2010-01-01", "2012-01-01")), date_labels = "%Y") +
theme_classic()
coverage %>% group_by(year) %>% summarise(spread = sum(spread)) %>%
ggplot(aes(year, spread)) +
geom_line() +
theme_classic()
coverage %>% group_by(year) %>% distinct(paper) %>% count() %>%
ggplot(aes(year, n)) +
geom_line() +
theme_classic()
# Match papers that have the exact same name.
papersCoverage <- data.frame(paper = tolower(unique(coverage$paper)), stringsAsFactors = FALSE)
papersDatabase <- data.frame(paper = unique(database$paper), stringsAsFactors = FALSE)
matchedPapers <- inner_join(papersCoverage, papersDatabase)
matchedPapers <- data.frame(s1name = matchedPapers$paper, s2name = matchedPapers$paper, stringsAsFactors = FALSE)
papersCoverage <- papersCoverage %>% filter(!paper %in% matchedPapers$s1name)
papersDatabase <- papersDatabase %>% filter(!paper %in% matchedPapers$s1name)
# Match using a variety of string distance measurements
emptyVector1 <- c(rep(0, length(papersDatabase$paper)))
paperNames <- data.frame(papersDatabase$paper, emptyVector1, stringsAsFactors = F)
names(paperNames) <- "name"
emptyVector2 <- c(rep(0, length(papersCoverage$paper)))
namesDekning <- data.frame(papersCoverage$paper, emptyVector2, stringsAsFactors = F)
names(namesDekning)[1] <- "name"
#distance.methods<-'jw'
distance.methods<-c('osa','lv','dl','lcs','qgram','cosine','jaccard','jw')
dist.methods<-list()
for (m in 1:length(distance.methods)) {
dist.name.enh<-matrix(NA, ncol = length(paperNames$name), nrow = length(namesDekning$name))
for (i in 1:length(paperNames$name)) {
for (j in 1:length(namesDekning$name)) {
dist.name.enh[j, i] <- stringdist(tolower(paperNames[i, ]$name),
tolower(namesDekning[j, ]$name), method = distance.methods[m])
#adist.enhance(paperNames[i,]$name,namesDekning[j,]$name)
}
}
dist.methods[[distance.methods[m]]] <- dist.name.enh
}
